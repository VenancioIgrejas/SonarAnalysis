{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple using MLP multi-class with boosting and majority vote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading Dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading Data] done in 0.00124597549438\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gc\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print('[{0}] done in {1}'.format(name,time.time() - t0))\n",
    "    \n",
    "\n",
    "with timer('Loading Data'):\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform dataset in dataframe\n",
    "\n",
    "df_data = pd.DataFrame(data=X,columns=iris.feature_names)\n",
    "df_trgt = pd.DataFrame(data=y,columns=['target'])\n",
    "df = pd.concat([df_data,df_trgt],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "from Functions.MLPClassification import MLPSKlearn,MLPhep\n",
    "#from hep_ml.nnet import MLPClassifier\n",
    "from hep_ml.nnet import MLPMultiClassifier\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier,VotingClassifier\n",
    "\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(40,),\n",
    "#                    activation='tanh',\n",
    "#                    solver='adam',\n",
    "#                    alpha=0.0001, \n",
    "#                    batch_size='auto',\n",
    "#                    learning_rate='constant',\n",
    "#                    learning_rate_init=0.001,\n",
    "#                    power_t=0.5,\n",
    "#                    max_iter=1000,\n",
    "#                    shuffle=True,\n",
    "#                    random_state=None,\n",
    "#                    tol=0.01,\n",
    "#                    verbose=True,\n",
    "#                    warm_start=False,\n",
    "#                    momentum=0.9,\n",
    "#                    nesterovs_momentum=True,\n",
    "#                    early_stopping=True,\n",
    "#                    validation_fraction=0.2,\n",
    "#                    beta_1=0.9,\n",
    "#                    beta_2=0.999,\n",
    "#                    epsilon=1e-08)\n",
    "\n",
    "#neural = MLPClassifier(layers=[10], trainer='adadelta', trainer_parameters={'batch': 600})\n",
    "#mlp = MLPMultiClassifier(layers=(10,), scaler='standard', trainer='irprop-', epochs=100,\n",
    "#                trainer_parameters=None, random_state=None)\n",
    "\n",
    "mlp = MLPSKlearn()\n",
    "#mlp = MLPhep()\n",
    "\n",
    "estimator = 5\n",
    "classifier = AdaBoostClassifier(base_estimator=mlp,\n",
    "                                n_estimators=estimator,\n",
    "                                algorithm='SAMME.R')\n",
    "#mlp = MLPClassifier()\n",
    "#mj_vot = VotingClassifier(tuple(classifier.estimators_),n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] 1 of 1 inits\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0030 - acc: 0.0083\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0029 - acc: 0.0083\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0029 - acc: 0.0250\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0028 - acc: 0.0333\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0028 - acc: 0.0500\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0027 - acc: 0.0500\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0027 - acc: 0.0667\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 26us/step - loss: 0.0026 - acc: 0.1167\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0026 - acc: 0.1417\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0026 - acc: 0.1750\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0025 - acc: 0.2000\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0025 - acc: 0.2250\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0024 - acc: 0.2583\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0024 - acc: 0.2667\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0023 - acc: 0.2667\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0023 - acc: 0.2750\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0023 - acc: 0.2833\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0022 - acc: 0.2917\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0022 - acc: 0.3000\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0021 - acc: 0.3083\n",
      "[+] 1 of 1 inits\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 0.4750\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0017 - acc: 0.4667\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0017 - acc: 0.4833\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0016 - acc: 0.5333\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 31us/step - loss: 0.0015 - acc: 0.5583\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.5833\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6000\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0013 - acc: 0.5833\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 0.5833\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 0.5833\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0012 - acc: 0.5917\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0012 - acc: 0.6167\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 0.6250\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 26us/step - loss: 0.0011 - acc: 0.6417\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0011 - acc: 0.6500\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 26us/step - loss: 0.0010 - acc: 0.6500\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0010 - acc: 0.6583\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 9.7398e-04 - acc: 0.6583\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 21us/step - loss: 9.4848e-04 - acc: 0.6667\n",
      "[+] 1 of 1 inits\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0017 - acc: 0.4167\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0016 - acc: 0.4500\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0016 - acc: 0.4750\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0015 - acc: 0.4750\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0015 - acc: 0.4917\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0015 - acc: 0.5250\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.5333\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.5417\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 0.5667\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0013 - acc: 0.5750\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0013 - acc: 0.6000\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0012 - acc: 0.6167\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 33us/step - loss: 0.0012 - acc: 0.6417\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0012 - acc: 0.6500\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0012 - acc: 0.6500\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0011 - acc: 0.6583\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 0.6667\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0011 - acc: 0.6917\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0011 - acc: 0.7000\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0011 - acc: 0.7000\n",
      "[+] 1 of 1 inits\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0020 - acc: 0.1667\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 31us/step - loss: 0.0019 - acc: 0.2083\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0019 - acc: 0.2583\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0018 - acc: 0.2917\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 31us/step - loss: 0.0018 - acc: 0.3333\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0017 - acc: 0.3833\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0017 - acc: 0.3750\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0016 - acc: 0.4167\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0016 - acc: 0.4333\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.4667\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.5167\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 26us/step - loss: 0.0015 - acc: 0.5333\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0014 - acc: 0.5500\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.5667\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6000\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 0.6417\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0013 - acc: 0.6750\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 0.6833\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0012 - acc: 0.7167\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0012 - acc: 0.7083\n",
      "[+] 1 of 1 inits\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0019 - acc: 0.2667\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 22us/step - loss: 0.0019 - acc: 0.2750\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0019 - acc: 0.2917\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0018 - acc: 0.3000\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0018 - acc: 0.3083\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 29us/step - loss: 0.0017 - acc: 0.3167\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 26us/step - loss: 0.0017 - acc: 0.3167\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0017 - acc: 0.3333\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 32us/step - loss: 0.0016 - acc: 0.3583\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 0.3833\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0016 - acc: 0.4167\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0015 - acc: 0.4417\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0015 - acc: 0.5167\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0015 - acc: 0.5750\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 30us/step - loss: 0.0015 - acc: 0.6333\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6583\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6667\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6750\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 0.6750\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 28us/step - loss: 0.0014 - acc: 0.6750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=MLPSKlearn(activation=None, alpha=None, batch_size=None, beta_1=None,\n",
       "      beta_2=None, dir=None, early_stopping=None, epsilon=None,\n",
       "      hidden_layer_sizes=None, learning_rate=None, learning_rate_init=None,\n",
       "      max_iter=None, momentum=None, n_iter_no_change=None,\n",
       "      nesterovs_momentum=None, power_t=None, shuffle=None, solver=None,\n",
       "      tol=None, validation_fraction=None, verbose=None, warm_start=None),\n",
       "          learning_rate=1.0, n_estimators=5, random_state=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usingo only MLP simple \n",
    "#mlp.fit(data_preproc_train,y_train)\n",
    "\n",
    "#using MLP with boosting\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classifier.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0]\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "for est in classifier.estimators_:\n",
    "    print est.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-035251c266fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#using votMaj with boosting MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmj_vot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/venancio/sonarteste/lib/python2.7/site-packages/sklearn/ensemble/voting_classifier.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \"\"\"\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'estimators_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'soft'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mmaj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/venancio/sonarteste/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This VotingClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "mj_vot = VotingClassifier(zip(['est{0}'.format(i) for i in range(estimator)],\n",
    "                              classifier.estimators_))\n",
    "\n",
    "#using votMaj with boosting MLP\n",
    "mj_vot.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
